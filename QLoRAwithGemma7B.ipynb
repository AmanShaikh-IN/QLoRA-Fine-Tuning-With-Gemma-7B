{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d63d403-f914-44e5-ab04-85d4cd2e7e56",
   "metadata": {},
   "source": [
    "# QLoRA Based Fine Tuning with Gemma7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688b536c-063a-469d-8478-8ab859e12fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627ce61e-68bd-4ecd-929a-00184c2c2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading from transformers library via the traditional and xet api took way too long, downloaded it from hf and ran on local machine\n",
    "### Additionally google colab has python set to 3.12/3.14 which causes problems for this setup so I decided to try locally and maybe later deply on\n",
    "### huggingspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01869c42-7727-4eb1-ad1d-7f7f5af11123",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you're trying on local machine ensure, your performance mode is set up and only if you have a good gpu to prevent complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc387cbd-ad49-4fe9-8be3-ed0ea24ccb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU not available! Please enable GPU: Runtime -> Change runtime type -> T4 GPU\")\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46cf3259-8e80-4134-9cf0-7261d8872c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will remove secrets later when uploading notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdceddb-9245-40a2-a689-34c7d95dff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa45fb06-f764-4b20-9be1-eadbb5e1495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf101d-32ef-4209-a6ac-1ff351fe383b",
   "metadata": {},
   "source": [
    "## 4 Bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ef0cda-10ff-4f45-aa76-7a71120fbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9660bb3-6a33-4117-af09-9b8db5caf365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2932f52-4f03-4f95-96f8-2f3a2c9a192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4cad5c94744d43b45217fb4237724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/mnt/d/LangChainProjects/QLoRAWithGemma7B/models/gemma-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83bb84b-dfa2-4c9f-a4e0-54af4e1047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7249d3-f5f6-49fd-b01f-23ee078bb15f",
   "metadata": {},
   "source": [
    "## Establishing Pre Instruct Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb71510-315f-45bb-bea4-effb4c8863c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e403e00-9c9e-46ef-b5e1-68ebaba46439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output:\n",
      "Quote: Imagination is more important than knowledge.\n",
      "\n",
      "Imagine a place where you can create whatever you want.  Imagination is such a powerful force.  It is the only place that has no rules, no boundaries.  It is a place like no other.  A place where anything can happen.  This is what makes imagination so special.  This is why imagination is more important than knowledge.\n",
      "\n",
      "<strong>Knowledge</strong>\n",
      "\n",
      "Knowledge is information.  It is what you learn by reading books, by listening to experts,\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Quote: A woman is like a tea bag; she steeps in hot water and develops into something beautiful.\n",
      "\n",
      "We, as a community, have a responsibility to uplift and motivate our women. We must show appreciation for the tremendous amount of work they do on a daily basis. This is the reason why this year, we decided to focus our attention on the women in our lives, and we chose to do this through a special event, The Woman of the Year Awards.\n",
      "\n",
      "The Woman of the Year Awards are a platform that celebrates, recognizes,\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model output:\")\n",
    "print(generate_text(\"Quote: Imagination is more\"))\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n\")\n",
    "print(generate_text(\"Quote: A woman is like a tea bag;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6b74df-f52b-463e-b9f6-87e627b45354",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c9e6c6-94d1-4baa-a7e4-c123e7531bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "def formatting_func(examples):\n",
    "    texts = []\n",
    "    for quote, author in zip(examples[\"quote\"], examples[\"author\"]):\n",
    "        text = f\"Quote: {quote}\\nAuthor: {author}\"\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff003b6-cf1d-4002-8071-e0edd12aae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-quotes-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    max_steps=100,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd2aeaa-6de1-43f9-8072-0d94fa76afdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00754c1b-299f-4bd6-987e-c5bbf08256bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 34:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>15.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>11.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>11.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>9.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>10.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>8.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.206200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=10.7096435546875, metrics={'train_runtime': 2049.7885, 'train_samples_per_second': 0.39, 'train_steps_per_second': 0.049, 'total_flos': 1845011904841728.0, 'train_loss': 10.7096435546875, 'epoch': 0.3189792663476874})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edbb32b9-fed5-4160-a4f9-c6e0062be5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/mnt/d/LangChainProjects/QLoRAWithGemma7B/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote: Imagination is more important than knowledge.”\n",
      "Author: Albert Einstein, German-born theoretical physicist, author, and recipient of the 1921 Nobel Prize in Physics for his contributions to the field of theoretical physics, particularly his work on the special and general theories of relativity.\n",
      "\n",
      "Author: Albert Einstein, German-born theoretical physicist, author, and recipient of the 1921 Nobel Prize in Physics for his contributions to the field of theoretical physics, particularly his work on the special and general theories of\n",
      "\n",
      "==================================================\n",
      "\n",
      "Quote: A woman is like a tea bag; you never know how strong it is until it is in hot water.\"\n",
      "Author: Eleanor Roosevelt, 1935, <em>The American Womenâ€™s Committee of the Second Pan-American Union</em>\n",
      "\n",
      "Author: Eleanor Roosevelt, <em>The American Womenâ€™s Committee of the Second Pan-American Union</em>\n",
      "Author: Eleanor Roosevelt, <em>The American Womenâ€™s Committee of the Second Pan-American Union</em>\n",
      "Author: John Stuart Mill, <em>The Sub\n",
      "\n",
      "==================================================\n",
      "\n",
      "Quote: Outside of a dog, a book is man's best friend. Inside of a dog, it's too dark to read.\n",
      "Author: Groucho Marx, American actor and comedian, in a letter to a friend, 1959\n",
      "\n",
      "Author: Groucho Marx, American actor and comedian, in a letter to a friend, 1959\n",
      "Author: Groucho Marx, American actor and comedian, in a letter to a friend, 1959\n",
      "Author: Groucho Marx, American actor and comedian,\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuned model output:\")\n",
    "print(generate_text(\"Quote: Imagination is more\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(generate_text(\"Quote: A woman is like a tea bag;\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(generate_text(\"Quote: Outside of a dog, a book is man's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de41d575-6e51-4195-b90c-c4ff72a36854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./llama-quotes-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./llama-quotes-finetuned-final\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daaa4d6-429c-4535-b119-d37fd43c8656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LoRA 4bit (py310)",
   "language": "python",
   "name": "lora-4bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
